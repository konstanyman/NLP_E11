{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TurkuNLP/intro-to-nlp/blob/master/exercise_11_top_acc.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MxqCYdbBaz5Z"
      },
      "source": [
        "# Exercise 11: Top-K accuracy\n",
        "\n",
        "In the lecture, we learned to align embedding spaces, which allowed us to \"compute\" word translations. We marveled at how well this worked, but did not\n",
        "really evaluate this properly, even though we do have a nice test set.\n",
        "\n",
        "In the exercise, your task will be to evaluate the method using the simple \"top-k accuracy\" metric. This is a simple metric, which measures whether the correct target is among the first K nearest neighbors. In other words for the pair of source-target words $w_{fin}, w_{eng}$ we consider the transfer successful, if $w_{eng}$ is among the K nearest neighbors of the embedding we obtain by transforming $w_{ fin}$ with the matrix $M$. Top K accuracy then is the proportion of successfully transferred pairs, out of all pairs, as a percentage.\n",
        "\n",
        "This can be achieved by a simple modification of the lecture code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "c3-FPp5MZjAS"
      },
      "outputs": [],
      "source": [
        "import gensim\n",
        "import os\n",
        "import wget\n",
        "import zipfile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "### functions\n",
        "\n",
        "# load file\n",
        "def download_file(url, filename):\n",
        "    if not os.path.exists(filename):\n",
        "        wget.download(url, filename)\n",
        "    else:\n",
        "        print(f\"{filename} already exists. Skipping download.\")\n",
        "\n",
        "# unzip\n",
        "def unzip_model_bin_files(zip_file, destination):\n",
        "    with zipfile.ZipFile(zip_file, 'r') as zip_ref:\n",
        "        # Get a list of all files in the zip archive\n",
        "        file_list = zip_ref.namelist()\n",
        "\n",
        "        # Filter out only the model.bin files\n",
        "        model_bin_files = [file_name for file_name in file_list if os.path.splitext(file_name)[1] == '.bin']\n",
        "\n",
        "        # Extract the model.bin files to the destination folder\n",
        "        for file_name in model_bin_files:\n",
        "            zip_ref.extract(file_name, destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "12.zip already exists. Skipping download.\n",
            "42.zip already exists. Skipping download.\n"
          ]
        }
      ],
      "source": [
        "download_file(\"http://dl.turkunlp.org/TKO_7095_2023/12.zip\", \"12.zip\")\n",
        "download_file(\"http://dl.turkunlp.org/TKO_7095_2023/42.zip\", \"42.zip\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "# unzip_model_bin_files(\"12.zip\", \"en.bin\")\n",
        "# unzip_model_bin_files(\"42.zip\", \"fi.bin\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL1rXODZOCYi"
      },
      "source": [
        "\n",
        "\n",
        "*   Now we can load the embeddings\n",
        "*   These are huge, but they are sorted by frequency, so we can easily limit ourselves to the top 100,000 words, which will be plenty enough for us\n",
        "*   This is maybe good to note, now we enter the territory of NLP models which count in the gigabytes in size\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "4DbFt1VOaDCu"
      },
      "outputs": [],
      "source": [
        "# This is how you load the trained embeddings\n",
        "# check the documentation\n",
        "# w2v embeddings are traditionlly distributed in one of two formats: a text form, and a binary form\n",
        "# The embeddings we downloaded above are in the binary form, so we need to indicate that when loading\n",
        "\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "wv_emb_en=KeyedVectors.load_word2vec_format(\"en.bin\", limit=100000, binary=True, encoding=\"utf-8\")\n",
        "wv_emb_fi=KeyedVectors.load_word2vec_format(\"fi.bin\", limit=100000, binary=True, encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEsD37OxeP2C"
      },
      "source": [
        "`KeyedVectors` documentation is here: https://radimrehurek.com/gensim/models/keyedvectors.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlcN0CJif1-0"
      },
      "source": [
        "# Cross-lingual transfer\n",
        "\n",
        "* Among the most impressive demonstrations of word embeddings\n",
        "* Given a list of word pairs between two languages (source and target), one can induce a mapping matrix $M$ which performs a linear transformation of the source language embeddings onto the target language embeddings\n",
        "* This basically \"aligns\" the source language embedding space onto the target language embedding space\n",
        "\n",
        "This is well worth testing! Let's try!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14hiMqfUGMZ4"
      },
      "source": [
        "## Bilingual dictionaries\n",
        "\n",
        "* There are many sources, for example this one:\n",
        "* https://github.com/codogogo/xling-eval\n",
        "* The associated paper is here: https://aclanthology.org/P19-1070/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "FPzz9inbqY23"
      },
      "outputs": [],
      "source": [
        "# Grab the data\n",
        "download_file(\"https://raw.githubusercontent.com/codogogo/xling-eval/master/bli_datasets/en-fi/yacle.test.freq.2k.en-fi.tsv\", \"yacle.test.freq.2k.en-fi.tsv\")\n",
        "download_file(\"https://raw.githubusercontent.com/codogogo/xling-eval/master/bli_datasets/en-fi/yacle.train.freq.5k.en-fi.tsv\", \"yacle.train.freq.5k.en-fi.tsv\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "mjUoV2lBpbJG"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('of', 'of'), ('ettÃ¤', 'to'), ('sisÃ¤Ã¤n', 'in'), ('varten', 'for'), ('on', 'is'), ('pÃ¤Ã¤llÃ¤', 'on'), ('ettÃ¤', 'that'), ('mennessÃ¤', 'by'), ('TÃ¤mÃ¤', 'this'), ('kanssa', 'with')]\n",
            "5000\n",
            "[('omistautuminen', 'dedication'), ('toiveet', 'desires'), ('hylÃ¤tty', 'dismissed'), ('psyykkinen', 'psychic'), ('halkeamia', 'cracks'), ('laitokset', 'establishments'), ('tehokkuus', 'efficacy'), ('arvovalta', 'prestige'), ('kokaiini', 'cocaine'), ('kiihtyi', 'accelerated')]\n",
            "2000\n"
          ]
        }
      ],
      "source": [
        "pairs_train=[] #These will be pairs of (source,target) i.e. (Finnish, English) words used to induce the matrix M\n",
        "pairs_test=[]  #same but for testing, so we should make sure there is absolutely no overlap between the train and test data\n",
        "               #let's do it so that not one word in the test is is seen in any capacity in the training data\n",
        "\n",
        "import csv\n",
        "\n",
        "def get_vectors(fname):\n",
        "    \"\"\"\n",
        "    Read the pairs from the file `fname`\n",
        "    \"\"\"\n",
        "    pairs=[]\n",
        "    with open(fname) as f:\n",
        "        r = csv.reader(f,delimiter=\"\\t\") #the file is a .tsv i.e. tab-separated-values\n",
        "        for en_word,fi_word in r:\n",
        "            #I will reverse the order here, go from Finnish as the source, to English as the target\n",
        "            #That way it will be easier to check how this works using English as the target, which we all understand\n",
        "            pairs.append((fi_word,en_word))\n",
        "        return pairs\n",
        "\n",
        "train_data=get_vectors(\"yacle.train.freq.5k.en-fi.tsv\")\n",
        "test_data=get_vectors(\"yacle.test.freq.2k.en-fi.tsv\")\n",
        "print(train_data[:10])\n",
        "print(len(train_data))\n",
        "print(test_data[:10])\n",
        "print(len(test_data))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikK7jb1hHUGF"
      },
      "source": [
        "## Get the embeddings\n",
        "\n",
        "* Now we have the word pairs\n",
        "* We need the embeddings, so we can build our S and T matrices\n",
        "* Not all words will be in our W2V embeddings\n",
        "* Plus, we want to be 100% sure there is absolutely no overlap between the training and test data\n",
        "* This means not one word seen in the training data will be in the test data\n",
        "* The general approach will be to gather the vectors into a list, and then vstack (vertical stack) these to get a 2D array, i.e. a matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "L6zMmPGDz0pV"
      },
      "outputs": [],
      "source": [
        "import numpy\n",
        "\n",
        "def build_arrays(pairs,emb1,emb2,avoid=set()):\n",
        "    \"\"\"\n",
        "    `pairs`: pairs of (fi,en) words\n",
        "    `emb1`: source side (here Finnish) embeddings\n",
        "    `emb2`: target side (here English) embeddings\n",
        "    `avoid`: a set of words to avoid/ignore (will be used when building test data, to avoid train data)\n",
        "    \"\"\"\n",
        "    vecs1,vecs2,filtered_pairs=[],[],[]  #vectors for source words, vectors for target words, and the word pairs themselves, i.e. three same-length lists\n",
        "    for w1,w2 in pairs: #Go over all pairs that we got\n",
        "        # check if both vectors are available, and none of the words is to be avoided\n",
        "        if w1 in emb1 and w2 in emb2 and w1 not in avoid and w2 not in avoid:\n",
        "            #passed!\n",
        "            vecs1.append(emb1[w1]) #source-side embedding, the KeyedVectors object can be queried as if it was a dictionary, returns the embedding as 1-dim array\n",
        "            vecs2.append(emb2[w2]) #target-side embeddings\n",
        "            filtered_pairs.append((w1,w2)) #remember the pair\n",
        "    #Now we vstack() which turns the lists of embeddings into 2-dim array\n",
        "    return numpy.vstack(vecs1),numpy.vstack(vecs2),filtered_pairs\n",
        "\n",
        "# Gather the train data first\n",
        "array_train_fi,array_train_en,pairs_train=build_arrays(train_data,wv_emb_fi,wv_emb_en)\n",
        "# Now build the set of all words seen in training, so we can avoid them when building the test set. Note that \"|\" is set union operator\n",
        "everything_in_train=set(s for s,t in pairs_train)|set(t for s,t in pairs_train)\n",
        "# Test data next, avoiding the words from the training data:\n",
        "array_test_fi,array_test_en,pairs_test=build_arrays(test_data,wv_emb_fi,wv_emb_en,avoid=everything_in_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "ntsAk7U3-SoN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Overlap between train pairs and test pairs: 0\n",
            "Overlap between train fi words and test fi words: 0\n",
            "Overlap between train en words and test en words: 0\n"
          ]
        }
      ],
      "source": [
        "# Let's be super-sure there absolutely is no overlap of any kind!\n",
        "print(\"Overlap between train pairs and test pairs:\",len(set(pairs_train) & set(pairs_test))) # & is set intersection operator, intersection between train and test should be empty\n",
        "src_train=set(src_w for src_w,tgt_w in pairs_train) #train source words\n",
        "tgt_train=set(tgt_w for src_w,tgt_w in pairs_train) #train target words\n",
        "src_test=set(src_w for src_w,tgt_w in pairs_test)   #test source words\n",
        "tgt_test=set(tgt_w for src_w,tgt_w in pairs_test)   #test target words\n",
        "print(\"Overlap between train fi words and test fi words:\",len(src_train & src_test))\n",
        "print(\"Overlap between train en words and test en words:\",len(tgt_train & tgt_test))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DfeAUHJQJDu9"
      },
      "source": [
        "## Mapping matrix\n",
        "* Next we need to induce the transformation matrix\n",
        "* I.e. implement the least-squares methods from the lecture\n",
        "* Surely GPT4 can write this for us:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "BW9-9iL11-0K"
      },
      "outputs": [],
      "source": [
        "# This code was written by GPT4, but in a bit of a twisted form, so I modified it\n",
        "# to better correspond to the formulae in the lecture\n",
        "\n",
        "def learn_transformation_matrix(source, target):\n",
        "    # Compute the pseudo-inverse of the source matrix\n",
        "    source_pseudo_inverse = numpy.linalg.pinv(source) # This implements (S^T S)^-1 S^T  needed in the least-squares formula in the lecture slides\n",
        "    # Compute the transformation matrix M using least squares method\n",
        "    M = numpy.matmul(source_pseudo_inverse,target)  #...and this multiplies by T from right completing the formula in the slides ... two lines(!)\n",
        "    return M\n",
        "\n",
        "# fi -> en matrix\n",
        "M=learn_transformation_matrix(array_train_fi,array_train_en)\n",
        "\n",
        "# Ha ha well that was easy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "D3wKwHKb2Umo"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Source (fi) shape (3718, 100)\n",
            "Target (en) shape (3718, 300)\n",
            "M shape (100, 300)\n"
          ]
        }
      ],
      "source": [
        "print(\"Source (fi) shape\",array_train_fi.shape)\n",
        "print(\"Target (en) shape\",array_train_en.shape)\n",
        "print(\"M shape\",M.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "BQLYDxra3bvW"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Transformed shape: (1054, 300)\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "0.0023250168"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# And now we transform the source (Finnish) test embeddings into the English embedding space\n",
        "# using the matrix M\n",
        "test_fi_transformed=numpy.matmul(array_test_fi,M) # This corresponds to SM in the lecture slides, i.e. source transformed by M to the target embedding space\n",
        "print(\"Transformed shape:\",test_fi_transformed.shape)\n",
        "numpy.square(numpy.subtract(test_fi_transformed, array_test_en)).mean() #This is the mean square error of the actual target, and the transformed source, looks small enough :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFvVBgyuKBcs"
      },
      "source": [
        "* So now we have the originally Finnish embeddings transformed into the English space\n",
        "* How can we evaluate?\n",
        "\n",
        "1. Go over the test word pairs (fi,en)\n",
        "2. Use the transformed Finnish embedding as a query into the English space\n",
        "3. List top-N English words which appear near this transformed embedding\n",
        "\n",
        "Conceptually, we go: \"Finnish word -> lookup to Finnish vector -> multiply by M to get English vector -> lookup English words nearby\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "Pnb2qINM0wAN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "toiveet (in English desires):\n",
            "[('desires', 0.6048240661621094), ('Certainly', 0.6019873023033142), ('importantly', 0.6007572412490845), ('qualities', 0.596856951713562), ('ideas', 0.5890296697616577), ('desire', 0.5869849920272827), ('perspectives', 0.5857775807380676), ('sense', 0.5856721997261047), ('indeed', 0.5739371180534363), ('notions', 0.5737997889518738)]\n",
            "    desires, Certainly, importantly, qualities, ideas, desire, perspectives, sense, indeed, notions\n",
            "\n",
            "psyykkinen (in English psychic):\n",
            "[('cognitive', 0.6844740509986877), ('physiological', 0.6756170988082886), ('behavioral', 0.670926034450531), ('physical', 0.6503170132637024), ('neurological', 0.6412773132324219), ('mental', 0.6365656852722168), ('disorders', 0.6286876201629639), ('therapy', 0.6276494860649109), ('interpersonal', 0.6237663626670837), ('empathy', 0.6237050294876099)]\n",
            "    cognitive, physiological, behavioral, physical, neurological, mental, disorders, therapy, interpersonal, empathy\n",
            "\n",
            "halkeamia (in English cracks):\n",
            "[('crevices', 0.6356827616691589), ('vegetation', 0.6249440312385559), ('surfaces', 0.6220377683639526), ('limestone', 0.6210106611251831), ('gullies', 0.6192687153816223), ('ridges', 0.6184322834014893), ('walls', 0.6177305579185486), ('surface', 0.6156796813011169), ('sediment', 0.6122860312461853), ('tiles', 0.6122254729270935)]\n",
            "    crevices, vegetation, surfaces, limestone, gullies, ridges, walls, surface, sediment, tiles\n",
            "\n",
            "kokaiini (in English cocaine):\n",
            "[('additives', 0.63803631067276), ('pesticides', 0.6306907534599304), ('substances', 0.6083385348320007), ('foods', 0.6070770621299744), ('carcinogenic', 0.6045204997062683), ('caffeine', 0.6038080453872681), ('drugs', 0.6020959615707397), ('medications', 0.5993767976760864), ('adulterated', 0.5992927551269531), ('chemicals', 0.5985022187232971)]\n",
            "    additives, pesticides, substances, foods, carcinogenic, caffeine, drugs, medications, adulterated, chemicals\n",
            "\n",
            "kiihtyi (in English accelerated):\n",
            "[('slowed', 0.5320308208465576), ('accelerated', 0.5261135697364807), ('exacerbated', 0.5187168717384338), ('worsened', 0.5184645056724548), ('spurred', 0.5116445422172546), ('fueled', 0.5114974975585938), ('slackened', 0.5053064227104187), ('escalated', 0.5016471743583679), ('surged', 0.5009024143218994), ('stagnated', 0.49879077076911926)]\n",
            "    slowed, accelerated, exacerbated, worsened, spurred, fueled, slackened, escalated, surged, stagnated\n",
            "\n",
            "huippu (in English pinnacle):\n",
            "[('magnificent', 0.6115471720695496), ('breathtaking', 0.5757989287376404), ('majestic', 0.5678502321243286), ('beautiful', 0.5615334510803223), ('gorgeous', 0.5583323836326599), ('marvelous', 0.5566180944442749), ('fabulous', 0.5525440573692322), ('ideal', 0.5522326827049255), ('perfect', 0.5519671440124512), ('awesome', 0.5458477735519409)]\n",
            "    magnificent, breathtaking, majestic, beautiful, gorgeous, marvelous, fabulous, ideal, perfect, awesome\n",
            "\n",
            "skandaali (in English scandal):\n",
            "[('embarrassment', 0.5586699843406677), ('shame', 0.55364590883255), ('revelation', 0.5400979518890381), ('backlash', 0.5392076969146729), ('outrage', 0.5348949432373047), ('irony', 0.5236599445343018), ('uproar', 0.5234390497207642), ('accusation', 0.5207378268241882), ('outraged', 0.5171398520469666), ('scandalous', 0.5115211606025696)]\n",
            "    embarrassment, shame, revelation, backlash, outrage, irony, uproar, accusation, outraged, scandalous\n",
            "\n",
            "vaaroista (in English dangers):\n",
            "[('dangers', 0.5867192149162292), ('inform', 0.55777508020401), ('warn', 0.5552992820739746), ('describing', 0.5385019779205322), ('consequences', 0.529812216758728), ('explaining', 0.528709352016449), ('reminded', 0.5229590535163879), ('implication', 0.5203909277915955), ('hazards', 0.5160256028175354), ('acknowledging', 0.5154876708984375)]\n",
            "    dangers, inform, warn, describing, consequences, explaining, reminded, implication, hazards, acknowledging\n",
            "\n",
            "merirosvo (in English pirate):\n",
            "[('creature', 0.5999671816825867), ('clown', 0.5935691595077515), ('warrior', 0.592304527759552), ('magician', 0.5890441536903381), ('serpent', 0.5835051536560059), ('lizard', 0.5825474262237549), ('dragon', 0.5798237323760986), ('rabbit', 0.5772719979286194), ('demon', 0.5753873586654663), ('frog', 0.5745841264724731)]\n",
            "    creature, clown, warrior, magician, serpent, lizard, dragon, rabbit, demon, frog\n",
            "\n",
            "aistit (in English senses):\n",
            "[('senses', 0.6688564419746399), ('strangeness', 0.66175776720047), ('intimacy', 0.6591207385063171), ('self-awareness', 0.6455979943275452), ('sensual', 0.6424542665481567), ('playfulness', 0.6417754888534546), ('spirituality', 0.6403846740722656), ('metaphors', 0.6365603804588318), ('sensory', 0.6349173188209534), ('creatures', 0.6258801221847534)]\n",
            "    senses, strangeness, intimacy, self-awareness, sensual, playfulness, spirituality, metaphors, sensory, creatures\n",
            "\n",
            "kilpailija (in English rival):\n",
            "[('competitor', 0.605781614780426), ('entrant', 0.5139460563659668), ('player', 0.5111780166625977), ('competitors', 0.5096585154533386), ('upstart', 0.4913272559642792), ('performer', 0.4876869320869446), ('manufacturer', 0.48749664425849915), ('buyer', 0.48649847507476807), ('company', 0.4828502833843231), ('competes', 0.4826820492744446)]\n",
            "    competitor, entrant, player, competitors, upstart, performer, manufacturer, buyer, company, competes\n",
            "\n",
            "beige (in English beige):\n",
            "[('satin', 0.7588971853256226), ('shades', 0.7561382055282593), ('fuchsia', 0.7530307173728943), ('chartreuse', 0.7510851621627808), ('off-white', 0.7506507635116577), ('mauve', 0.7473527789115906), ('purple', 0.7466060519218445), ('lace', 0.7410832047462463), ('burgundy', 0.7375743389129639), ('lavender', 0.7370845675468445)]\n",
            "    satin, shades, fuchsia, chartreuse, off-white, mauve, purple, lace, burgundy, lavender\n",
            "\n",
            "nouseminen (in English boarding):\n",
            "[('lessening', 0.5342666506767273), ('consequence', 0.5199987292289734), ('importantly', 0.5118842720985413), ('push', 0.5110827088356018), ('undoubtedly', 0.5078465342521667), ('consequently', 0.5066491961479187), ('diminish', 0.5064816474914551), ('especially', 0.505935549736023), ('coming', 0.5049853920936584), ('lessen', 0.5005025863647461)]\n",
            "    lessening, consequence, importantly, push, undoubtedly, consequently, diminish, especially, coming, lessen\n",
            "\n",
            "suku (in English genus):\n",
            "[('genus', 0.5957473516464233), ('lineage', 0.5865923166275024), ('species', 0.5800135135650635), ('ancestor', 0.5730530619621277), ('offspring', 0.5713822841644287), ('domesticated', 0.5564213991165161), ('subspecies', 0.5465307831764221), ('descendants', 0.5450079441070557), ('ancestry', 0.5448456406593323), ('ancestors', 0.5447736978530884)]\n",
            "    genus, lineage, species, ancestor, offspring, domesticated, subspecies, descendants, ancestry, ancestors\n",
            "\n",
            "tarttuva (in English catching):\n",
            "[('affliction', 0.6243669390678406), ('bacterial', 0.6186230182647705), ('mutant', 0.6141065955162048), ('skin', 0.6040289402008057), ('parasites', 0.6022565364837646), ('fungus', 0.588144838809967), ('icky', 0.5849234461784363), ('viral', 0.5844663381576538), ('mutated', 0.5818915367126465), ('weird', 0.5791387557983398)]\n",
            "    affliction, bacterial, mutant, skin, parasites, fungus, icky, viral, mutated, weird\n",
            "\n"
          ]
        }
      ],
      "source": [
        "for i,(w1,w2) in enumerate(pairs_test[:10]):\n",
        "    print(f\"{w1} (in English {w2}):\")\n",
        "    nearest_neighbors=wv_emb_en.similar_by_vector(test_fi_transformed[i]) #lookup the nearest\n",
        "    print(nearest_neighbors)\n",
        "    #nearest_neigbors will be tuples (word,similarity_value)\n",
        "    eng_words=[w for w, score in nearest_neighbors] #just grab the words\n",
        "    print(f\"   \",\", \".join(eng_words)) #...and print then ,-separated\n",
        "    print()\n",
        "\n",
        "# It cannot be stressed enough, that none of the words in the test data were seen\n",
        "# during the induction of the transformation matrix M\n",
        "# \n",
        "# We can observe some direct top-1 hits, and in general we see\n",
        "# the mapping maps the vector very close to the correct place\n",
        "# in my view, this is quite impressive :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9lKMCAK6dS3G"
      },
      "source": [
        "# Put your exercise code here\n",
        "\n",
        "* You can start by copying and repurposing the code above. After all, if you can print the target and the nearest neigbors, you can also count them :)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ITca98IYdKIg"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top-1 accuracy is 18.216318785578746%\n",
            "Top-5 accuracy is 35.38899430740038%\n",
            "Top-10 accuracy is 43.07400379506641%\n",
            "Top-50 accuracy is 61.29032258064516%\n"
          ]
        }
      ],
      "source": [
        "########## YOUR CODE HERE #############\n",
        "\n",
        "for k in (1, 5, 10, 50):\n",
        "    i = 0\n",
        "    for j, (fi_w, en_w) in enumerate(pairs_test):\n",
        "        nearest_neighbors=wv_emb_en.similar_by_vector(test_fi_transformed[j], topn=k)\n",
        "        en_words=[w for w, score in nearest_neighbors]\n",
        "        if en_w in en_words:\n",
        "            i += 1\n",
        "    top_k_acc = (i/len(pairs_test))*100\n",
        "    print(f\"Top-{k} accuracy is {top_k_acc}%\")\n",
        "\n",
        "######## evaluate for K=1,5,10,50 for example #######\n",
        "########  below is what I got #########\n",
        "##\n",
        "##Top-1 accurracy is 17.82101167315175%\n",
        "##Top-5 accurracy is 34.007782101167315%\n",
        "##Top-10 accurracy is 42.4124513618677%\n",
        "##Top-50 accurracy is 61.08949416342413%"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMy3jVb45YceNTLzPkqbyHt",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
